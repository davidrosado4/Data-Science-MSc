{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9f241b-901f-4022-85e2-30b69733a99c",
   "metadata": {},
   "source": [
    "# Utils - Àlex Pujol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d70a1b-f7b0-4495-8aa3-695c5584ece5",
   "metadata": {},
   "source": [
    "This notebook contains the functions and its explanations built by Àlex Pujol for the Quora Questions task for NLP subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda40ce-4d43-4305-a224-0a9c1359c6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from utils import words_count, tokenize_text, remove_accents, remove_punctuation\n",
    "\n",
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8398b759-696d-475c-abd9-41e2f5697925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example questions to test the functions\n",
    "questions = [\n",
    "    \"I like to read books\", \"Reading books is enjoyable for me\",\n",
    "    \"She runs every morning\", \"Every morning she goes for a run\",\n",
    "    \"The cat is sleeping\", \"The sleeping cat is cute\",\n",
    "    \"I am learning to code\", \"Coding is a useful skill to learn\",\n",
    "    \"He enjoys playing video games\", \"Playing video games is his favorite hobby\",\n",
    "    \"The car stopped abruptly\", \"The abrupt stop of the car was surprising\",\n",
    "    \"We went to the beach\", \"The beach was crowded and sunny\",\n",
    "    \"She sings beautifully\", \"Her beautiful singing voice is captivating\",\n",
    "    \"The restaurant serves delicious food\", \"The food at the restaurant is always tasty\",\n",
    "    \"He is studying for an exam\", \"Studying is important for academic success\",\n",
    "    \"The flowers are blooming\", \"The blooming flowers are a sign of spring\",\n",
    "    \"The movie was entertaining\", \"I found the movie to be quite enjoyable\",\n",
    "    \"She is a talented musician\", \"Music is her passion and she is very talented\",\n",
    "    \"The building is very tall\", \"The tall building is an impressive feat of engineering\",\n",
    "    \"He traveled to Europe last summer\", \"Last summer he went on a trip to Europe\",\n",
    "    \"I love spending time with my family\", \"My family is very important to me\",\n",
    "    \"The book was very suspenseful\", \"I found the book to be quite thrilling\",\n",
    "    \"She enjoys painting and drawing\", \"Art is her favorite form of self-expression\",\n",
    "    \"The sun is shining brightly today\", \"The bright sun is making everything look beautiful\",\n",
    "    \"He is an excellent chef\", \"Cooking is his passion and he is very skilled\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561cd2a1-d30c-4b56-9290-d9029fa9cfa3",
   "metadata": {},
   "source": [
    "## Feature: Count Syllables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d744dbff-d615-4639-bc8e-06e18e45216d",
   "metadata": {},
   "source": [
    "Bellow are some functions to count syllables from words and from sentences. Useful as a feature itself and to build more complex features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95484dcd-6d93-4dbb-95d3-2d98f98129dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_syllables(word):\n",
    "    ''' \n",
    "    Args: \n",
    "        word (str): a tokenized word from a sentence.\n",
    "        \n",
    "    Return:\n",
    "        int: number of syllables from a word.\n",
    "    '''\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower().strip(\".:;?!\")\n",
    "    if word[0] in vowels:\n",
    "        count +=1\n",
    "    for index in range(1,len(word)):\n",
    "        if word[index] in vowels and word[index-1] not in vowels:\n",
    "            count +=1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if word.endswith('le'):\n",
    "        count+=1\n",
    "    if count == 0:\n",
    "        count +=1\n",
    "    return int(count)\n",
    "\n",
    "def count_sentence_syllables(doc):\n",
    "    '''\n",
    "    Args: \n",
    "        doc (str): a raw sentence.\n",
    "        \n",
    "    Return:\n",
    "        int: number of syllables of the entire sentence.\n",
    "    '''\n",
    "    count = 0\n",
    "    for w in tokenize_text(remove_accents(remove_punctuation(doc))):\n",
    "        count += count_word_syllables(w)\n",
    "    return int(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b517a81-07f7-4ac7-ad4a-0181ca7ee39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "ex = questions[random.randint(0,len(questions)-1)]\n",
    "print(\"Example sentence: \\n=>\",ex)\n",
    "print()\n",
    "print(\"Syllables for each word: \\n=>\", [count_word_syllables(w) for w in tokenize_text(remove_accents(remove_punctuation(ex)))])\n",
    "print()\n",
    "print(\"Total amount of syllables in the sentence: \\n=>\", count_sentence_syllables(ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df750ef9-9243-4ca5-a1ec-456b044c3932",
   "metadata": {},
   "source": [
    "## Feature: Readibility metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7efdb-19b0-4a4c-ac91-8f949f4d2fac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Flesch–Kincaid readability tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe31b93-b4ef-4717-8889-ec5fdb49cc73",
   "metadata": {
    "tags": []
   },
   "source": [
    "The **Flesch–Kincaid readability tests** are readability tests designed to indicate how difficult a passage in English is to understand. There are two tests: the Flesch Reading-Ease, and the Flesch–Kincaid Grade Level. Although they use the same core measures (word length and sentence length), they have different weighting factors. \n",
    "- Flesch Reading-Ease: Higher scores indicate material that is easier to read; lower numbers mark passages that are more difficult to read.\n",
    "- Flesch–Kincaid grade level: Presents a score as a U.S. grade level, making it easier for teachers, parents, librarians, and others to judge the readability level of various books and texts. It can also mean the number of years of education generally required to understand this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57281dd-24f9-4c19-9a97-c5f54cf74725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flesch Reading-Ease\n",
    "def Flesch_Reading_Ease(doc, a = 206.835, b = 1.015, c = 84.6):\n",
    "    '''\n",
    "    Args:\n",
    "        doc (str): sentence to analize.\n",
    "        a (float): Flesch Reading-Ease parameter.\n",
    "        b (float): Flesch Reading-Ease parameter.\n",
    "        c (float): Flesch Reading-Ease parameter.\n",
    "    \n",
    "    Return:\n",
    "        str: Computes the Flesch Reading-Ease score of the sentence\n",
    "    '''\n",
    "    return a - b * (words_count(doc) / 1) - c * (count_sentence_syllables(doc) / words_count(doc))\n",
    "\n",
    "\n",
    "# Flesch-Kincaid Grade Level\n",
    "def Flesch_Grade_Level(doc, a = 0.39, b = 11.8, c = 15.59):\n",
    "    '''\n",
    "    Args:\n",
    "        doc (str): sentence to analize.\n",
    "        a (float): Flesch-Kincaid Grade Level parameter.\n",
    "        b (float): Flesch-Kincaid Grade Level parameter.\n",
    "        c (float): Flesch-Kincaid Grade Level parameter.\n",
    "    \n",
    "    Return:\n",
    "        str: Computes the Flesch-Kincaid Grade Level score of the sentence.\n",
    "    '''\n",
    "    return a * (words_count(doc) / 1) + b * (count_sentence_syllables(doc) / words_count(doc)) - c\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d504a29f-043f-4d9a-babb-6f14b4ee9d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "ex = questions[random.randint(0,len(questions)-1)]\n",
    "print(f\"Readibility metrics for sentence: {ex}\\n\")\n",
    "print(f\"=> Flesch Reading-Ease score: {Flesch_Reading_Ease(ex)}\\n\")\n",
    "print(f\"=> Flesch-Kincaid Grade Level score: {Flesch_Grade_Level(ex)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b4d84-7e9c-4ad5-a00d-7943cd6c4ece",
   "metadata": {},
   "source": [
    "## Feature: Linguistic Features\n",
    "We make use of spaCy library to retrive different linguistic annotations of each question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5653f2ea-d56e-47eb-8247-2e8dc83ab070",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linguistics():\n",
    "    '''\n",
    "    Makes use of scapy library to extract linguistic features form sentences to either use them as features themselves or to build more complex features.\n",
    "    '''\n",
    "    def __init__(self, doc):\n",
    "        '''\n",
    "        Args:\n",
    "            doc (str): sentence to analyze.\n",
    "        '''\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.doc = doc\n",
    "        self.tokens = nlp(doc)\n",
    "    \n",
    "    def text(self):\n",
    "        '''\n",
    "        Tokenizes the sentence.\n",
    "        '''\n",
    "        return [token.text for token in self.tokens]\n",
    "    \n",
    "    def lemma(self):\n",
    "        '''\n",
    "        Lemmatizes the sentence.\n",
    "        '''\n",
    "        return [token.lemma_ for token in self.tokens]\n",
    "    \n",
    "    def pos(self):\n",
    "        '''\n",
    "        Applies simple Part-Of-Speech tagging to the sentence.\n",
    "        '''\n",
    "        return [token.pos_ for token in self.tokens]\n",
    "    \n",
    "    def tag(self):\n",
    "        '''\n",
    "        Applies detailed Part-Of-Speech tagging to the sentence.\n",
    "        '''\n",
    "        return [token.tag_ for token in self.tokens]\n",
    "    \n",
    "    def dep(self):\n",
    "        '''\n",
    "        Applies the syntactic dependencey between tokens in the sentence.\n",
    "        '''\n",
    "        return [token.dep_ for token in self.tokens]\n",
    "    \n",
    "    def shape(self):\n",
    "        '''\n",
    "        Applies tagging to words according to their shape.\n",
    "        '''\n",
    "        return [token.shape_ for token in self.tokens]\n",
    "    \n",
    "    def is_alpha(self):\n",
    "        '''\n",
    "        Applies tagging according for word being an Alpha token or not.\n",
    "        '''\n",
    "        return [token.is_alpha for token in self.tokens]\n",
    "    \n",
    "    def is_stop(self):\n",
    "        '''\n",
    "        Applies tagging according for word being a stopword or not.\n",
    "        '''\n",
    "        return [token.is_stop for token in self.tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea6ab47-ef1f-4cc1-9be2-ace382dc593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "ex = questions[random.randint(0,len(questions)-1)]\n",
    "print(f\"Linguistic Analisys for sentence: {ex}\\n\")\n",
    "\n",
    "a = Linguistics(ex)\n",
    "\n",
    "print(f\"=> Text: {a.text()}\\n\")\n",
    "print(f\"=> Lemmatization: {a.lemma()}\\n\")\n",
    "print(f\"=> POS: {a.pos()}\\n\")\n",
    "print(f\"=> Detailed POS: {a.tag()}\\n\")\n",
    "print(f\"=> Syntactic Dependencies: {a.dep()}\\n\")\n",
    "print(f\"=> Shape: {a.shape()}\\n\")\n",
    "print(f\"=> Alpha token: {a.is_alpha()}\\n\")\n",
    "print(f\"=> Stopwords: {a.is_stop()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec583d-b2c3-4121-9dc2-811ba9233ba1",
   "metadata": {},
   "source": [
    "## Feature: Compound Linguistic Features\n",
    "Using above class we can implement compoud linguistic features for the pair of questions, such as, chechking if they share the same ROOT, difference in stopwords, difference of nouns, gramatical complexity, and others that may come up as we try more things to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b844bf-cbce-411b-9219-743b25edcdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_root(doc1, doc2):\n",
    "    '''\n",
    "    Args:\n",
    "        doc1 (str): sentence to compare.\n",
    "        doc2 (str): sentence to compare.\n",
    "        \n",
    "    Return:\n",
    "        bool: True if sentences share the same root, False otherwise.\n",
    "    '''\n",
    "    a1 = Linguistics(doc1)\n",
    "    a2 = Linguistics(doc2)\n",
    "    \n",
    "    try:\n",
    "        root1 = a1.text()[a1.dep().index(\"ROOT\")]\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        root2 = a2.text()[a2.dep().index(\"ROOT\")]\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "    return root1 == root2\n",
    "\n",
    "def count_sentence_stopwords(doc):\n",
    "    '''\n",
    "    Args:\n",
    "        doc (str): sentence to analize.\n",
    "        \n",
    "    Return:\n",
    "        int: Amount of stopwords contained in sentence.\n",
    "    '''\n",
    "    a = Linguistics(doc)\n",
    "    \n",
    "    return sum(a.is_stop())\n",
    "\n",
    "    \n",
    "def diff_stopwords(doc1, doc2):\n",
    "    '''\n",
    "    Args:\n",
    "        doc1 (str): sentence to compare.\n",
    "        doc2 (str): sentence to compare.\n",
    "        \n",
    "    Return:\n",
    "        int: Difference in amount of stopwords between sentences.\n",
    "    '''\n",
    "    return abs(count_sentence_stopwords(doc1) - count_sentence_stopwords(doc2))\n",
    "\n",
    "    \n",
    "def count_sentence_nouns(doc):\n",
    "    '''\n",
    "    Args:\n",
    "        doc (str): sentence to analize.\n",
    "        \n",
    "    Return:\n",
    "        int: Amount of nouns contained in sentence.\n",
    "    '''\n",
    "    a = Linguistics(doc)\n",
    "    \n",
    "    try:\n",
    "        return a.pos().count(\"NOUN\")\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def diff_nouns(doc1, doc2):\n",
    "    '''\n",
    "    Args:\n",
    "        doc1 (str): sentence to compare.\n",
    "        doc2 (str): sentence to compare.\n",
    "        \n",
    "    Return:\n",
    "        int: Difference in amount of nouns between sentences.\n",
    "    '''\n",
    "    return abs(count_sentence_nouns(doc1) - count_sentence_nouns(doc2))\n",
    "\n",
    "    \n",
    "def gramatical_complexity_overlap(doc1, doc2):\n",
    "    '''\n",
    "    Args:\n",
    "        doc1 (str): sentence to compare.\n",
    "        doc2 (str): sentence to compare.\n",
    "        \n",
    "    Return:\n",
    "        int: Intersection of different gramatical types of words contained in sentences. That is how many times both sentences contain a noun, a verb, and so on at the same time.\n",
    "    '''\n",
    "    a1 = Linguistics(doc1)\n",
    "    a2 = Linguistics(doc2)\n",
    "    \n",
    "    return len(set(a1.tag()).intersection(set(a2.tag())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175c8c3c-887f-41a0-a3d7-d1ae834175a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "ex1 = questions[random.randint(0,len(questions)-1)]\n",
    "ex2 = questions[random.randint(0,len(questions)-1)]\n",
    "a1 = Linguistics(ex1)\n",
    "a2 = Linguistics(ex2)\n",
    "\n",
    "print(f\"Linguistic Analisys for sentences:\\n\\t---> {ex1} \\n\\t---> {ex2}\\n\\n\")\n",
    "\n",
    "print(f\"Do they share the same root? --> {same_root(ex1,ex2)}\\n\")\n",
    "\n",
    "print(f\"What is the difference in amount of stopwords? --> {diff_stopwords(ex1,ex2)}\\n\")\n",
    "\n",
    "print(f\"What is the difference in amount of nouns? --> {diff_nouns(ex1,ex2)}\\n\")\n",
    "\n",
    "print(a1.tag())\n",
    "print(f\"Gramatical complexity overlap: --> {gramatical_complexity_overlap(ex1, ex2)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615fbafc-2108-4e8e-a154-fbf5a5d004e8",
   "metadata": {},
   "source": [
    "## Functionality: TF-IDF implementation in cython\n",
    "The method `tf_idf.compute_tf_idf(str docs)` computes the tf-idf of `docs` and returns each document as a normalized tf-idf sparse array, `csr_array`. Also returns the vocabulary of `docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45fddbe-ad76-4272-9746-8d436225ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cython_utils import tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dbfe9b-1b1a-419f-8d9c-f8a79c8c3209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "import time\n",
    "start_time = time.time()\n",
    "tf_idfs, vocab = tf_idf.compute_tf_idf(questions)\n",
    "\n",
    "\n",
    "print(\"---Execution time: %s seconds ---\\n\\n\" % (time.time() - start_time))\n",
    "\n",
    "for i, doc_tf_idf in enumerate(tf_idfs[-2:]):\n",
    "    print(f\"Document {i}: {questions[-2+i]}\\n\")\n",
    "    print(f\"\\ttf_idf: {doc_tf_idf}\\n\")\n",
    "    \n",
    "    # To convert csr_array to array:\n",
    "    print(f\"\\tArray: {doc_tf_idf.toarray().reshape(-1)}\\n\")\n",
    "    \n",
    "    # To call a specific element:\n",
    "    print(f\"\\tElement 7 of the array: {doc_tf_idf[:,[6]].toarray().reshape(-1)[0]}\\n\\tCorresponds to word: {vocab[6]}\\n\\n\")\n",
    "\n",
    "print(f\"Vocabulary: {vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ade65f-e8ed-40d4-9812-5a3c048071b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
